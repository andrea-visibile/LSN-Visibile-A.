{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\"> Numerical Simulation Laboratory </span>\n",
    "## <span style=\"color:brown\"> Python Exercise 11 </span>\n",
    "## <span style=\"color:orange\"> Keras - Neural Network regression </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview \n",
    "\n",
    "In this notebook our task will be to perform machine learning regression on noisy data with a Neural Network (NN).\n",
    "\n",
    "We will explore how the ability to fit depends on the structure of the NN. The goal is also to build intuition about why prediction is difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Prediction Problem\n",
    "\n",
    "Consider a probabilistic process that gives rise to labeled data $(x,y)$. The data is generated by drawing samples from the equation\n",
    "\n",
    "$$\n",
    "    y_i= f(x_i) + \\eta_i,\n",
    "$$\n",
    "\n",
    "where $f(x_i)$ is some fixed, but (possibly unknown) function, and $\\eta_i$ is a Gaussian, uncorrelate noise variable such that\n",
    "\n",
    "$$\n",
    "\\langle \\eta_i \\rangle=0 \\\\\n",
    "\\langle \\eta_i \\eta_j \\rangle = \\delta_{ij} \\sigma\n",
    "$$\n",
    "\n",
    "We will refer to the $f(x_i)$ as the **true features** used to generate the data. \n",
    "\n",
    "To make prediction, we will consider a NN that depend on its parameters, weights and biases. The functions that the NN can model respresent the **model class** that we are using to try to model the data and make predictions.\n",
    "\n",
    "To learn the parameters of the NN, we will train our models on a **training data set** and then test the effectiveness of the NN on a *different* dataset, the **validation data set**. The reason we must divide our data into a training and test dataset is that the point of machine learning is to make accurate predictions about new data we have not seen.\n",
    "\n",
    "To measure our ability to predict, we will learn our parameters by fitting our training dataset and then making predictions on our test data set. One common measure of predictive  performance of our algorithm is to compare the predictions,$\\{y_j^\\mathrm{pred}\\}$, to the true values $\\{y_j\\}$. A commonly employed measure for this is the sum of the mean square-error (MSE) on the test set:\n",
    "$$\n",
    "MSE= \\frac{1}{N_\\mathrm{test}}\\sum_{j=1}^{N_\\mathrm{test}} (y_j^\\mathrm{pred}-y_j)^2\n",
    "$$\n",
    "\n",
    "We will try to get a qualitative picture by examining plots on validation and training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear fit\n",
    "\n",
    "We start by considering the very simple case:\n",
    "$$\n",
    "f(x)=2x+1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start defining the parameters of an ideal linear function which we are going to predict through a neural network regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target parameters of f(x) = m*x + b\n",
    "m = 2 # slope\n",
    "b = 1 # intersect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate a set of input data which will slightly deviate from our ideal behaviour using a random noise (that actually is set to zero):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate training inputs\n",
    "np.random.seed(0)\n",
    "x_train = np.random.uniform(-1, 1, 100)\n",
    "x_valid = np.random.uniform(-1, 1, 10)\n",
    "x_valid.sort()\n",
    "y_target = m * x_valid + b # ideal (target) linear function\n",
    "\n",
    "sigma = 0.0 # noise standard deviation, for the moment it is absent\n",
    "y_train = np.random.normal(m * x_train + b, sigma) # actual measures from which we want to guess regression parameters\n",
    "y_valid = np.random.normal(m * x_valid + b, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training and target dataset\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_valid, y_target)\n",
    "plt.scatter(x_valid, y_valid, color='r')\n",
    "plt.grid(True); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you remember how a single node of a neural network works, you can easily spot that **just a single neuron can make the job**. So let's start using a simple Sequential model with just one layer on one neuron only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose the NN model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(1, input_shape=(1,)))\n",
    "\n",
    "# compile the model choosing optimizer, loss and metrics objects\n",
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a summary of our composed model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to train our model, that is we feed the neuron with the set of training pair x, y_train from which the optimizer will find the best weights to minimize the Mean Square Error loss function (out linear regression function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model using training dataset\n",
    "# over 10 epochs of 32 batch size each\n",
    "# report training progress against validation data\n",
    "history = model.fit(x=x_train, y=y_train, \n",
    "          batch_size=32, epochs=30,\n",
    "          shuffle=True, # a good idea is to shuffle input before at each epoch\n",
    "          validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return weights\n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "score = model.evaluate(x_valid, y_valid, batch_size=32, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model with the exact curve\n",
    "score = model.evaluate(x_valid, y_target, batch_size=32, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into training history\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predicted = np.random.uniform(-1, 1, 100)\n",
    "y_predicted = model.predict(x_predicted)\n",
    "plt.scatter(x_predicted, y_predicted,color='r')\n",
    "plt.plot(x_valid, y_target)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.1\n",
    "\n",
    "In order to make practice with NN, explore how does the previous linear regression depend on the number of epochs, $N_{\\mathrm{epochs}}$, the number of data points $N_{\\mathrm{train}}$ and on the noise $\\sigma$. Try to improve the previous result operating on these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.2\n",
    "\n",
    "Try to extend the model to obtain a reasonable fit of the following polynomial of order 3:\n",
    "\n",
    "$$\n",
    "f(x)=4-3x-2x^2+3x^3\n",
    "$$\n",
    "for $x \\in [-1,1]$.\n",
    "\n",
    "In order to make practice with NN, explore reasonable different choices for:\n",
    "\n",
    "- the number of layers\n",
    "- the number of neurons in each layer\n",
    "- the activation function\n",
    "- the optimizer\n",
    "- the loss function\n",
    "  \n",
    "Make graphs comparing fits for different NNs.\n",
    "Check your NN models by seeing how well your fits predict newly generated test data (including on data outside the range you fit. How well do your NN do on points in the range of $x$ where you trained the model? How about points outside the original training data set? \n",
    "Summarize what you have learned about the relationship between model complexity (number of parameters), goodness of fit on training data, and the ability to predict well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.3\n",
    "  \n",
    "Vogliamo ora estendere il modello a:\n",
    "$$f(x,y) = \\sin(x^2+y^2)$$\n",
    "nell'intervallo $x \\in [-3/2,3/2]$ e $y \\in [-3/2,3/2]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas to meditate on these exercises and judge your results can be found <a href=https://xkcd.com/2048/>here</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "class PlotCurrentEstimate(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, x_valid, y_valid):\n",
    "        \"\"\"Keras Callback which plot current model estimate against reference target\"\"\"\n",
    "        \n",
    "        # convert numpy arrays into lists for plotting purposes\n",
    "        self.x_valid = list(x_valid[:])\n",
    "        self.y_valid = list(y_valid[:])\n",
    "        self.iter=0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        temp = self.model.predict(self.x_valid, batch_size=None, verbose=False, steps=None)\n",
    "        self.y_curr = list(temp[:]) # convert numpy array into list\n",
    "        \n",
    "        self.iter+=1\n",
    "        if self.iter%10 == 0:\n",
    "            clear_output(wait=True)            \n",
    "            self.eplot = plt.subplot(1,1,1)\n",
    "            self.eplot.clear()     \n",
    "            self.eplot.scatter(self.x_valid, self.y_curr, color=\"blue\", s=4, marker=\"o\", label=\"estimate\")\n",
    "            self.eplot.scatter(self.x_valid, self.y_valid, color=\"red\", s=4, marker=\"x\", label=\"valid\")\n",
    "            self.eplot.legend()\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use also an EarlyStopping callback on the val_loss quantity. This will stop the training process as soon as the val_loss quantity does not improve anymore after an amount of epochs, preventing a long time of wated computation to take over without useful results.\n",
    "\n",
    "<code>keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)</code>\n",
    "\n",
    "Arguments:\n",
    "\n",
    "- <code>monitor</code>: quantity to be monitored. \n",
    "- <code>min_delta:</code> minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement. \n",
    "- <code>patience:</code> number of epochs with no improvement after which training will be stopped. \n",
    "- <code>verbose:</code> verbosity mode. \n",
    "- <code>mode:</code> one of {auto, min, max}. In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. \n",
    "- <code>baseline:</code> Baseline value for the monitored quantity to reach. Training will stop if the model doesn't show improvement over the baseline. \n",
    "- <code>restore_best_weights:</code> whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD8CAYAAACW/ATfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmWXA/we4Sjl9mmGQ7QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.0118 - mean_squared_error: 0.0118 - val_loss: 0.0117 - val_mean_squared_error: 0.0117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[1.7890505]], dtype=float32), array([1.0447811], dtype=float32)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_estimate = PlotCurrentEstimate(x_valid, y_valid)\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                  min_delta=0, patience=100, mode='auto')\n",
    "\n",
    "model.fit(x_valid, y_valid, batch_size=32, epochs=150,\n",
    "          validation_data=(x_valid, y_valid),\n",
    "          callbacks=[ plot_estimate, earlystop]\n",
    "          )\n",
    "\n",
    "model.get_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
